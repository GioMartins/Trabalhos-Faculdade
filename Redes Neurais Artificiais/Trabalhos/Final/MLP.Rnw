\documentclass{article}

\begin{document}
\SweaveOpts{concordance=TRUE}

<<>>=
rm(list = ls())
library('RSNNS')
library(mlbench)
library(plot3D)
library(corpcor)

normalize <- function(x) {
  return((x - min(x)) / (max(x) - min(x)))
}

@

<<>>=
# Dataset 1: Breast Cancer

data("BreastCancer")
BreastCancer <- BreastCancer[complete.cases(BreastCancer),]

# Separacao dos Dados do Dataset
X <- data.matrix(BreastCancer[,(2:10)])

Y_tanh <- data.matrix(2*(as.numeric(BreastCancer[,11])-1.5))
Y_sigm <- data.matrix(as.numeric(BreastCancer[,11]))
Y_sigm <- Y_sigm / max(Y_sigm)
Y_relu <- data.matrix(as.numeric(BreastCancer[,11]))

# Divisao dos dados em treinamento e teste
set.seed(123)
training_index <- sample(1:nrow(BreastCancer), 0.7 * nrow(BreastCancer))
test_index <- setdiff(1:nrow(BreastCancer), training_index)

# Separacao e normalizacao dos dados de entrada para treinamento e teste
X_training <- X[training_index,]
X_test <- X[test_index,]
X_training <- apply(X_training, 2, normalize)
X_test <- apply(X_test, 2, normalize)

# Separacao das saidas para as diferentes funcoes de ativacao
Y_training_tanh <- Y_tanh[training_index]
Y_test_tanh <- Y_tanh[test_index]

Y_training_sigm <- Y_sigm[training_index]
Y_test_sigm <- Y_sigm[test_index]

Y_training_relu <- Y_relu[training_index]
Y_test_relu <- Y_relu[test_index]
@

<<>>=
# Dataset 2: Pima Indians Diabetes

data("PimaIndiansDiabetes2")
PimaIndiansDiabetes2 <- PimaIndiansDiabetes2[complete.cases(PimaIndiansDiabetes2),]

# Separacao dos Dados do Dataset
X <- data.matrix(PimaIndiansDiabetes2[,(1:8)])

Y_tanh <- data.matrix(2*(as.numeric(PimaIndiansDiabetes2[,9])-1.5))
Y_sigm <- data.matrix(as.numeric(PimaIndiansDiabetes2[,9]))
Y_sigm <- Y_sigm / max(Y_sigm)
Y_relu <- data.matrix(as.numeric(PimaIndiansDiabetes2[,9]))

# Divisao dos dados em treinamento e teste
set.seed(123)
training_index <- sample(1:nrow(PimaIndiansDiabetes2), 0.7 * nrow(PimaIndiansDiabetes2))
test_index <- setdiff(1:nrow(PimaIndiansDiabetes2), training_index)

# Separacao e normalizacao dos dados de entrada para treinamento e teste
X_training <- X[training_index,]
X_test <- X[test_index,]
X_training <- apply(X_training, 2, normalize)
X_test <- apply(X_test, 2, normalize)

# Separacao das saidas para as diferentes funcoes de ativacao
Y_training_tanh <- Y_tanh[training_index]
Y_test_tanh <- Y_tanh[test_index]

Y_training_sigm <- Y_sigm[training_index]
Y_test_sigm <- Y_sigm[test_index]

Y_training_relu <- Y_relu[training_index]
Y_test_relu <- Y_relu[test_index]
@

<<>>=
# Dataset3: SPECT

SPECT_test <- read.csv("C:/Projetos/Trabalhos-Faculdade/Redes Neurais Artificiais/Trabalhos/Final/SPECT.test", header=FALSE)
SPECT_train <- read.csv("C:/Projetos/Trabalhos-Faculdade/Redes Neurais Artificiais/Trabalhos/Final/SPECT.train", header=FALSE)

X_training <- SPECT_train[,2:23]
X_test <- SPECT_test[,2:23]
X_training <- apply(X_training, 2, normalize)
X_test <- apply(X_test, 2, normalize)
Y_training <- SPECT_train[,1]
Y_test <- SPECT_test[,1]

Y_training_tanh <- Y_training
Y_test_tanh <- Y_test
Y_training_sigm <- Y_training
Y_test_sigm <- Y_test
Y_training_relu <- Y_training
Y_test_relu <- Y_test


# Separacao das saidas de treinamento e teste para as diferentes funcoes de ativacao
Y_training_tanh <- ifelse(Y_training_tanh == 0, -1, Y_training_tanh)
Y_training_tanh <- ifelse(Y_training_tanh == 1, 1, Y_training_tanh)
Y_training_sigm <- ifelse(Y_training_sigm == 0, 0.5, Y_training_sigm)
Y_training_sigm <- ifelse(Y_training_sigm == 1, 1, Y_training_sigm)
Y_training_relu <- ifelse(Y_training_relu == 1, 2, Y_training_relu)
Y_training_relu <- ifelse(Y_training_relu == 0, 1, Y_training_relu)


Y_test_tanh <- ifelse(Y_test_tanh == 0, -1, Y_test_tanh)
Y_test_tanh <- ifelse(Y_test_tanh == 1, 1, Y_test_tanh)
Y_test_sigm <- ifelse(Y_test_sigm == 0, 0.5, Y_test_sigm)
Y_test_sigm <- ifelse(Y_test_sigm == 1, 1, Y_test_sigm)
Y_test_relu <- ifelse(Y_test_relu == 1, 2, Y_test_relu)
Y_test_relu <- ifelse(Y_test_relu == 0, 1, Y_test_relu)
@

<<>>=
# Dataset 4: Ionosphere

ionosphere <- read.csv("C:/Projetos/Trabalhos-Faculdade/Redes Neurais Artificiais/Trabalhos/Final/ionosphere.data", header=FALSE)

# Separacao dos Dados do Dataset
X <- data.matrix(ionosphere[,(1:34)])
Y <- data.matrix(ionosphere[,35])

# Divisao dos dados em treinamento e teste
set.seed(123)
training_index <- sample(1:nrow(ionosphere), 0.7 * nrow(ionosphere))
test_index <- setdiff(1:nrow(ionosphere), training_index)

# Separacao dos dados de entrada e saida para treinamento e teste
X_training <- X[training_index,]
X_test <- X[test_index,]

Y_training_tanh <- Y[training_index,]
Y_test_tanh <- Y[test_index,]
Y_training_sigm <- Y[training_index,]
Y_test_sigm <- Y[test_index,]
Y_training_relu <- Y[training_index,]
Y_test_relu <- Y[test_index,]

# Ajuste das saidas para as funcoes de ativacao
Y_training_tanh <- ifelse(Y_training_tanh == "b", -1, Y_training_tanh)
Y_training_tanh <- ifelse(Y_training_tanh == "g", 1, Y_training_tanh)

Y_training_sigm <- ifelse(Y_training_sigm == "b", 0.5, Y_training_sigm)
Y_training_sigm <- ifelse(Y_training_sigm == "g", 1, Y_training_sigm)

Y_training_relu <- ifelse(Y_training_relu == "b", 1, Y_training_relu)
Y_training_relu <- ifelse(Y_training_relu == "g", 2, Y_training_relu)

Y_test_tanh <- ifelse(Y_test_tanh == "b", -1, Y_test_tanh)
Y_test_tanh <- ifelse(Y_test_tanh == "g", 1, Y_test_tanh)

Y_test_sigm <- ifelse(Y_test_sigm == "b", 0.5, Y_test_sigm)
Y_test_sigm <- ifelse(Y_test_sigm == "g", 1, Y_test_sigm)

Y_test_relu <- ifelse(Y_test_relu == "b", 1, Y_test_relu)
Y_test_relu <- ifelse(Y_test_relu == "g", 2, Y_test_relu)

# Conversao para o valor numerico
Y_training_tanh <- as.numeric(Y_training_tanh)
Y_training_sigm <- as.numeric(Y_training_sigm)
Y_training_relu <- as.numeric(Y_training_relu)

Y_test_tanh <- as.numeric(Y_test_tanh)
Y_test_sigm <- as.numeric(Y_test_sigm)
Y_test_relu <- as.numeric(Y_test_relu)
@

<<>>=
# Dataset 5: WILT

training_wilt <- read.csv("C:/Projetos/Trabalhos-Faculdade/Redes Neurais Artificiais/Trabalhos/Final/training_wilt.csv")
testing_wilt <- read.csv("C:/Projetos/Trabalhos-Faculdade/Redes Neurais Artificiais/Trabalhos/Final/testing_wilt.csv")

# Separacao dos dados de entrada
X_training <- data.matrix(training_wilt[,2:6])
X_test <- data.matrix(testing_wilt[,2:6])
X_training <- apply(X_training, 2, normalize)
X_test <- apply(X_test, 2, normalize)

# Separacao dos dados de saida
Y_training <- data.matrix(training_wilt[,1])
Y_test <- data.matrix(testing_wilt[,1])

Y_training_tanh <- Y_training
Y_test_tanh <- Y_test
Y_training_sigm <- Y_training
Y_test_sigm <- Y_test
Y_training_relu <- Y_training
Y_test_relu <- Y_test


# Ajuste das saidas para as funcoes de ativacao
Y_training_tanh <- ifelse(Y_training_tanh == "n", -1, Y_training_tanh)
Y_training_tanh <- ifelse(Y_training_tanh == "w", 1, Y_training_tanh)

Y_training_sigm <- ifelse(Y_training_sigm == "n", 0.5, Y_training_sigm)
Y_training_sigm <- ifelse(Y_training_sigm == "w", 1, Y_training_sigm)

Y_training_relu <- ifelse(Y_training_relu == "n", 1, Y_training_relu)
Y_training_relu <- ifelse(Y_training_relu == "w", 2, Y_training_relu)

Y_test_tanh <- ifelse(Y_test_tanh == "n", -1, Y_test_tanh)
Y_test_tanh <- ifelse(Y_test_tanh == "w", 1, Y_test_tanh)

Y_test_sigm <- ifelse(Y_test_sigm == "n", 0.5, Y_test_sigm)
Y_test_sigm <- ifelse(Y_test_sigm == "w", 1, Y_test_sigm)

Y_test_relu <- ifelse(Y_test_relu == "n", 1, Y_test_relu)
Y_test_relu <- ifelse(Y_test_relu == "w", 2, Y_test_relu)

# Conversao para o valor numerico
Y_training_tanh <- as.numeric(Y_training_tanh)
Y_training_sigm <- as.numeric(Y_training_sigm)
Y_training_relu <- as.numeric(Y_training_relu)

Y_test_tanh <- as.numeric(Y_test_tanh)
Y_test_sigm <- as.numeric(Y_test_sigm)
Y_test_relu <- as.numeric(Y_test_relu)
@





<<>>=

# Montagem da arquitetura
number_neurons <- c(1,5,10,15,20,30, 50, 100) # Numero de neuronios em cada camada oculta
M <- matrix(0, nrow=length(number_neurons), ncol=6)

for (i in 1:length(number_neurons)) {
  n <- number_neurons[i]
  
  
  # TANGENTE HIPERBOLICA
  model_tanh <- mlp(X_training, Y_training_tanh, size = c(n), maxit = 20000, initFunc = "Randomize_Weights", 
             initFuncParams = c(-5, 5),
             learnFunc = "Std_Backpropagation", learnFuncParams = c(0.001, 0.5),
             updateFunc = "Topological_Order", updateFuncParams = c(0),
             hiddenActFunc = "Act_Tanh",
             shufflePatterns = TRUE, linOut = TRUE)
  
  # Previsao do conjunto de treinamento
  Y_training_prediction_tanh <- predict(model_tanh, X_training)
  Training_Error_tanh <- Y_training_prediction_tanh - Y_training_tanh
  
  # Previsao do conjunto de teste
  Y_test_prediction_tanh <- predict(model_tanh, X_test)
  Test_Error_tanh <- Y_test_prediction_tanh - Y_test_tanh
  
  # Calculo do erro medio e do desvio padrao
  MSE_Training_Error_tanh <- mean(Training_Error_tanh^2)
  MSE_Test_Error_tanh <- mean(Test_Error_tanh^2)
  SD_Training_Error_tanh <- sqrt(MSE_Training_Error_tanh)
  SD_Test_Error_thanh <- sqrt(MSE_Test_Error_tanh)
  #-----------------------------------------------------------------------------------------------------------
  

  # SIGMOIDE
  model_sigm <- mlp(X_training, Y_training_sigm, size = c(n), maxit = 20000, initFunc = "Randomize_Weights", 
             initFuncParams = c(-5, 5),
             learnFunc = "Std_Backpropagation", learnFuncParams = c(0.001, 0.5),
             updateFunc = "Topological_Order", updateFuncParams = c(0),
             hiddenActFunc = "Act_Logistic",
             shufflePatterns = TRUE, linOut = TRUE)

  # Previsao do conjunto de treinamento
  Y_training_prediction_sigm <- predict(model_sigm, X_training)
  Training_Error_sigm <- Y_training_prediction_sigm - Y_training_sigm
  
  # Previsao do conjunto de teste
  Y_test_prediction_sigm <- predict(model_sigm, X_test)
  Test_Error_sigm <- Y_test_prediction_sigm - Y_test_sigm
  
  # Calculo do erro medio e do desvio padrao
  MSE_Training_Error_sigm <- mean(Training_Error_sigm^2)
  MSE_Test_Error_sigm <- mean(Test_Error_sigm^2)
  SD_Training_Error_sigm <- sqrt(MSE_Training_Error_sigm)
  SD_Test_Error_sigm <- sqrt(MSE_Test_Error_sigm)
  #-----------------------------------------------------------------------------------------------------------
  
  
  # RELU
  model_relu <- mlp(X_training, Y_training_relu, size = c(n), maxit = 20000, initFunc = "Randomize_Weights", 
             initFuncParams = c(-5, 5),
             learnFunc = "Std_Backpropagation", learnFuncParams = c(0.001, 0.5),
             updateFunc = "Topological_Order", updateFuncParams = c(0),
             hiddenActFunc = "Act_Threshold",
             shufflePatterns = TRUE, linOut = TRUE)
  
  # Previsao do conjunto de treinamento
  Y_training_prediction_relu <- predict(model_relu, X_training)
  Training_Error_relu <- Y_training_prediction_relu - Y_training_relu
  
  # Previsao do conjunto de teste
  Y_test_prediction_relu <- predict(model_relu, X_test)
  Test_Error_relu <- Y_test_prediction_relu - Y_test_relu
  
  # Calculo do erro medio e do desvio padrao
  MSE_Training_Error_relu <- mean(Training_Error_relu^2)
  MSE_Test_Error_relu <- mean(Test_Error_relu^2)
  SD_Training_Error_relu <- sqrt(MSE_Training_Error_relu)
  SD_Test_Error_relu <- sqrt(MSE_Test_Error_relu)
  #-----------------------------------------------------------------------------------------------------------
  
  
  # Resultados
  cat("Numero de Neuronios: ", n, "\n")

  M[i, 1] <- MSE_Training_Error_tanh
  M[i, 2] <- MSE_Test_Error_tanh
  M[i, 3] <- MSE_Training_Error_sigm
  M[i, 4] <- MSE_Test_Error_sigm
  M[i, 5] <- MSE_Training_Error_relu
  M[i, 6] <- MSE_Test_Error_relu
}
cat("Acabou! \n")
@


<<>>=
# PLOTANDO ERROS DE TESTE

plot(number_neurons, M[,2], type='l', lwd= 3, col='red', ylim = c(0, max(M)), ylab = 'M (Erro - MLP)', xlab = 'Numero de Neuronios')
par(new = T)
plot(number_neurons, M[,4], type='l', lwd= 3, col='blue', ylim = c(0, max(M)), ylab = 'M (Erro - MLP)', xlab = 'Numero de Neuronios')
par(new = T)
plot(number_neurons, M[,6], type='l', lwd= 3, col='green', ylim = c(0, max(M)), ylab = 'M (Erro - MLP)', xlab = 'Numero de Neuronios')
par(new = T)
legend(x = "topright",legend=c("Erro de Teste - Tahn", 
                               "Erro de Teste - Sigm", 
                               "Erro de Teste - Relu"), 
                               fill=c("red", "blue", "green"))
@


<<>>=
# PLOTANDO ERROS DE TREINAMENTO E TESTE

plot(number_neurons, M[,1], type='l', lwd= 3, col='red',, ylim = c(0, max(M)), ylab = 'M (Erro)', xlab = 'Numero de Neuronios' )
par(new=T)
plot(number_neurons, M[,2], type='l', lwd= 3, col='pink', ylim = c(0, max(M)), ylab = 'M (Erro)', xlab = 'Numero de Neuronios')
par(new = T)
plot(number_neurons, M[,3], type='l', lwd= 3, col='black',, ylim = c(0, max(M)), ylab = 'M (Erro)', xlab = 'Numero de Neuronios' )
par(new=T)
plot(number_neurons, M[,4], type='l', lwd= 3, col='gray', ylim = c(0, max(M)), ylab = 'M (Erro)', xlab = 'Numero de Neuronios')
par(new = T)
plot(number_neurons, M[,5], type='l', lwd= 3, col='blue',, ylim = c(0, max(M)), ylab = 'M (Erro)', xlab = 'Numero de Neuronios' )
par(new=T)
plot(number_neurons, M[,6], type='l', lwd= 3, col='lightblue', ylim = c(0, max(M)), ylab = 'M (Erro)', xlab = 'Numero de Neuronios')
par(new = T)
legend(x = "topright",legend=c("Erro de Treinamento - Tahn", "Erro de Teste - Tahn", 
                               "Erro de Treinamento - Sigm", "Erro de Teste - Sigm", 
                               "Erro de Treinamento - Relu", "Erro de Teste - Relu"), 
                               fill=c("red", "pink", "black", "gray", "blue", "lightblue"))


@



\end{document}
